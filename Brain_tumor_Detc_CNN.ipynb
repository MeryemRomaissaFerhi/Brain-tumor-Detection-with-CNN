{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hgYy0iODd0ZT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn import svm, linear_model, ensemble, neighbors, tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMt1A82XByuG",
        "outputId": "f3c4159c-62e6-43a7-d39b-04eaa26d4104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muea2hbsceuE",
        "outputId": "feb3f373-d244-4d8c-d9d7-e91f5853db84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No Tumor:  1500\n",
            "Tumor:  1500\n"
          ]
        }
      ],
      "source": [
        "datasets = \"/content/drive/MyDrive/datasets\"\n",
        "\n",
        "# Lister les images dans les sous-répertoires\n",
        "no_tumor_images = os.listdir(os.path.join(datasets, 'no'))\n",
        "yes_tumor_images = os.listdir(os.path.join(datasets, 'yes'))\n",
        "\n",
        "dataset=[]\n",
        "label=[]\n",
        "\n",
        "INPUT_SIZE=224\n",
        "\n",
        "print('No Tumor: ', len(no_tumor_images))\n",
        "print('Tumor: ',len(yes_tumor_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cLSYIbVoc6fO"
      },
      "outputs": [],
      "source": [
        "for image_name in no_tumor_images:\n",
        "    if image_name.split('.')[-1].lower() == 'jpg':\n",
        "        image_path = os.path.join(datasets, 'no', image_name)\n",
        "        try:\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is not None:\n",
        "                image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "                image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
        "                dataset.append(np.array(image))\n",
        "                label.append(0)\n",
        "            else:\n",
        "                print(f\"Could not read image: {image_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {str(e)}\")\n",
        "\n",
        "for image_name in yes_tumor_images:\n",
        "    if image_name.split('.')[-1].lower() == 'jpg':\n",
        "        image_path = os.path.join(datasets, 'yes', image_name)\n",
        "        try:\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is not None:\n",
        "                image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "                image = image.resize((INPUT_SIZE, INPUT_SIZE))\n",
        "                dataset.append(np.array(image))\n",
        "                label.append(1)\n",
        "            else:\n",
        "                print(f\"Could not read image: {image_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Ui10ovoilmX"
      },
      "outputs": [],
      "source": [
        "dataset = np.array(dataset)\n",
        "label = np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eJYpn9ouixqh"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ulLZ2uH0i1Dc"
      },
      "outputs": [],
      "source": [
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6Hql3Sk7HQ2",
        "outputId": "9022f712-a133-450c-d452-35a42e3535c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 86528)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               22151424  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22244929 (84.86 MB)\n",
            "Trainable params: 22244929 (84.86 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Définir le modèle CNN\n",
        "model = Sequential()\n",
        "\n",
        "# 1ère couche de convolution et de pooling\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 2ème couche de convolution et de pooling\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 3ème couche de convolution et de pooling\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Aplatir les résultats pour le réseau fully connected\n",
        "model.add(Flatten())\n",
        "\n",
        "# Couche fully connected avec dropout\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Couche de sortie\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Résumé du modèle\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIcishaz7qM7",
        "outputId": "bad22700-7326-4ff1-a88c-cb1ed6d72d50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "75/75 [==============================] - ETA: 0s - loss: 0.5341 - accuracy: 0.7525"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r75/75 [==============================] - 324s 4s/step - loss: 0.5341 - accuracy: 0.7525 - val_loss: 0.4496 - val_accuracy: 0.7933\n",
            "Epoch 2/20\n",
            "75/75 [==============================] - 346s 5s/step - loss: 0.3493 - accuracy: 0.8517 - val_loss: 0.3424 - val_accuracy: 0.8650\n",
            "Epoch 3/20\n",
            "75/75 [==============================] - 324s 4s/step - loss: 0.2529 - accuracy: 0.8971 - val_loss: 0.3087 - val_accuracy: 0.8900\n",
            "Epoch 4/20\n",
            "75/75 [==============================] - 332s 4s/step - loss: 0.1787 - accuracy: 0.9350 - val_loss: 0.2476 - val_accuracy: 0.9150\n",
            "Epoch 5/20\n",
            "75/75 [==============================] - 357s 5s/step - loss: 0.1404 - accuracy: 0.9558 - val_loss: 0.2424 - val_accuracy: 0.9167\n",
            "Epoch 6/20\n",
            "75/75 [==============================] - 359s 5s/step - loss: 0.1106 - accuracy: 0.9658 - val_loss: 0.1779 - val_accuracy: 0.9367\n",
            "Epoch 7/20\n",
            "75/75 [==============================] - 348s 5s/step - loss: 0.0847 - accuracy: 0.9762 - val_loss: 0.1681 - val_accuracy: 0.9450\n",
            "Epoch 8/20\n",
            "75/75 [==============================] - 334s 4s/step - loss: 0.0514 - accuracy: 0.9862 - val_loss: 0.1592 - val_accuracy: 0.9500\n",
            "Epoch 9/20\n",
            "75/75 [==============================] - 362s 5s/step - loss: 0.0457 - accuracy: 0.9879 - val_loss: 0.1458 - val_accuracy: 0.9617\n",
            "Epoch 10/20\n",
            "75/75 [==============================] - 324s 4s/step - loss: 0.0311 - accuracy: 0.9921 - val_loss: 0.1512 - val_accuracy: 0.9550\n",
            "Epoch 11/20\n",
            "75/75 [==============================] - 357s 5s/step - loss: 0.0244 - accuracy: 0.9937 - val_loss: 0.1492 - val_accuracy: 0.9633\n",
            "Epoch 12/20\n",
            "75/75 [==============================] - 344s 5s/step - loss: 0.0255 - accuracy: 0.9917 - val_loss: 0.1842 - val_accuracy: 0.9583\n",
            "Epoch 13/20\n",
            "75/75 [==============================] - 344s 5s/step - loss: 0.0166 - accuracy: 0.9962 - val_loss: 0.1559 - val_accuracy: 0.9600\n",
            "Epoch 14/20\n",
            "75/75 [==============================] - 337s 5s/step - loss: 0.0114 - accuracy: 0.9987 - val_loss: 0.1578 - val_accuracy: 0.9617\n",
            "Epoch 15/20\n",
            "75/75 [==============================] - 340s 5s/step - loss: 0.0123 - accuracy: 0.9962 - val_loss: 0.1752 - val_accuracy: 0.9617\n",
            "Epoch 16/20\n",
            "75/75 [==============================] - 320s 4s/step - loss: 0.0091 - accuracy: 0.9983 - val_loss: 0.1595 - val_accuracy: 0.9600\n",
            "Epoch 17/20\n",
            "75/75 [==============================] - 314s 4s/step - loss: 0.0100 - accuracy: 0.9979 - val_loss: 0.2226 - val_accuracy: 0.9550\n",
            "Epoch 18/20\n",
            "75/75 [==============================] - 333s 4s/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.1994 - val_accuracy: 0.9667\n",
            "Epoch 19/20\n",
            "75/75 [==============================] - 333s 4s/step - loss: 0.0059 - accuracy: 0.9992 - val_loss: 0.1994 - val_accuracy: 0.9600\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Définir les callbacks\n",
        "checkpoint = ModelCheckpoint('best_model_cnn.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Entraîner le modèle\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sxbp9VC7tF9",
        "outputId": "1717e7a2-9fb1-4399-8f9b-cba7644dc608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19/19 [==============================] - 25s 1s/step - loss: 0.1994 - accuracy: 0.9667\n",
            "Test Accuracy: 96.67%\n"
          ]
        }
      ],
      "source": [
        "# Charger le meilleur modèle sauvegardé\n",
        "model.load_weights('best_model_cnn.h5')\n",
        "\n",
        "# Évaluer le modèle\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdu5DXOHD6zU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
